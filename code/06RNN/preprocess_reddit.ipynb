{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tutorial : WildML :\n",
    "#http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/ \n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano \n",
    "import theano.tensor as T\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "#### Tokens ####\n",
    "vocabulary_size = 8000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed sentences : 79170\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "##### Add sentence start and end tokens ###\n",
    "with open('../../data/reddit-comments_raw.csv','rb') as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # split comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    # convert sentences into form (START,sentence,END)\n",
    "    sentences = ['%s %s %s' %(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "print 'parsed sentences : %d'%(len(sentences))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle the preprocessed sentences \n",
    "import cPickle\n",
    "#cPickle.dump(sentences, open('../../data/reddit_comments_preprocessed.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded = cPickle.load(open('../../data/reddit_comments_preprocessed.pkl', 'rb'))\n",
    "sentences = loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unique words : 65751\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences to words\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "# get the frequencies of words\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "# print number of unique words\n",
    "print '#unique words : %d' %(len(word_freq))\n",
    "# get the most common 8K words\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index2word = [x[0] for x in vocab]\n",
    "index2word.append(unknown_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2index = dict([(w,i) for i,w in enumerate(index2word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times.\n"
     ]
    }
   ],
   "source": [
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word2index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word2index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word2index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "#X_train = np.asarray([[word2index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "#y_train = np.asarray([[word2index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "cPickle.dump((X_train,y_train), open('../../data/reddit_training_set.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cPickle.dump((tokenized_sentences,vocab,word2index,index2word),open('../../data/reddit_metadata.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
