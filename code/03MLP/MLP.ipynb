{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960\n"
     ]
    }
   ],
   "source": [
    "### >> Multilayer Perceptron << ###\n",
    "## http://deeplearning.net/tutorial/mlp.html ##\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "from logistic import load_data,LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the hidden layer class\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self,rng,input,n_in,n_out,w=None,b=None):\n",
    "        self.input = input\n",
    "        if w is None:\n",
    "            wval = np.asarray(rng.uniform(low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                    high=np.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)),\n",
    "                              dtype = theano.config.floatX)\n",
    "            w = theano.shared(wval,name='w',borrow=True)\n",
    "        #bias\n",
    "        if b is None:\n",
    "            b = theano.shared(\n",
    "                value = np.zeros( (n_out,),dtype=theano.config.floatX),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "        \n",
    "        #b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        #b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "        \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.output = T.tanh(T.dot(input,self.w)+self.b)\n",
    "        self.params = [self.w,self.b]    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n"
     ]
    }
   ],
   "source": [
    "datasets = load_data('mnist.pkl.gz')\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "# compute number of minibatches for training, validation and testing\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### : Testing the hidden layer ###\n",
    "#------------------------------##\n",
    "\n",
    "x = T.matrix('x')\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "hl = HiddenLayer(rng, input=x, n_in=28*28, n_out=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define MLP class ###\n",
    "class MLP(object):\n",
    "    def __init__(self,rng,input,n_in,n_h,n_out):\n",
    "        self.hidden_layer = HiddenLayer(rng,input=input,n_in=n_in,n_out=n_h)\n",
    "        self.output_layer = LogisticRegression(input=self.hidden_layer.output, n_in=n_h,n_out=n_out)\n",
    "        #regularization\n",
    "        self.L1 = abs(self.hidden_layer.w).sum() + abs(self.output_layer.w).sum()\n",
    "        self.L2 = (self.hidden_layer.w**2).sum() + (self.output_layer.w**2).sum()\n",
    "        # Negative Log Likelihood\n",
    "        self.neg_log_likelihood = (self.output_layer.neg_log_likelihood)\n",
    "        # errors function\n",
    "        self.errors = (self.output_layer.errors)\n",
    "        # params\n",
    "        self.params = self.hidden_layer.params + self.output_layer.params\n",
    "        \n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = T.lscalar('index')\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "# instantiate MLP classifier\n",
    "cl = MLP(rng, input = x, n_in = 28*28, n_h = 500, n_out = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup cost\n",
    "cost = cl.neg_log_likelihood(y) + (cl.L1 * 0.00) + (cl.L2 * 0.0001)\n",
    "\n",
    "# setup gradient\n",
    "gparams = [ T.grad(cost,param) for param in cl.params ]\n",
    "\n",
    "# setup updates \n",
    "updates = [ (param, param - 0.01*gparam) for param,gparam in zip(cl.params,gparams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile training function\n",
    "train = theano.function(inputs=[index],\n",
    "                       outputs=cost,\n",
    "                       updates=updates,\n",
    "                       givens = { x : train_set_x[index * batch_size : (index+1)*batch_size],\n",
    "                                  y : train_set_y[index * batch_size : (index+1)*batch_size]\n",
    "                                }\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0  : cost :  0.0806066840887\n",
      "iteration  10  : cost :  0.0784379839897\n",
      "iteration  20  : cost :  0.0765156522393\n",
      "iteration  30  : cost :  0.0747864171863\n",
      "iteration  40  : cost :  0.073216073215\n",
      "iteration  50  : cost :  0.0717805102468\n",
      "iteration  60  : cost :  0.0704620331526\n",
      "iteration  70  : cost :  0.0692474022508\n",
      "iteration  80  : cost :  0.0681257024407\n",
      "iteration  90  : cost :  0.0670884475112\n"
     ]
    }
   ],
   "source": [
    "# Actual training begins here\n",
    "minibatch_avg_cost = 0\n",
    "for j in xrange(100):\n",
    "    for i in xrange(n_train_batches):\n",
    "        minibatch_avg_cost = train(i)\n",
    "    if j % 10 == 0:\n",
    "        print 'iteration ',j,' : cost : ', minibatch_avg_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile the test function\n",
    "test = theano.function(inputs=[index],\n",
    "                      outputs=cl.errors(y),\n",
    "                      givens = { x : test_set_x[index*batch_size : (index+1)*batch_size],\n",
    "                                 y : test_set_y[index*batch_size : (index+1)*batch_size]\n",
    "                               }\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_error :  0.0198\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "error_sum = 0.0\n",
    "for i in xrange(n_test_batches):\n",
    "    error_sum += test(i)\n",
    "print 'avg_error : ',error_sum/n_test_batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
