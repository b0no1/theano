{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### >> Multilayer Perceptron << ###\n",
    "## http://deeplearning.net/tutorial/mlp.html ##\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "from logistic import load_data,LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the hidden layer class\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self,rng,input,n_in,n_out,w=None,b=None):\n",
    "        self.input = input\n",
    "        if w is None:\n",
    "            wval = np.asarray(rng.uniform(low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                    high=np.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)),\n",
    "                              dtype = theano.config.floatX)\n",
    "            w = theano.shared(wval,name='w',borrow=True)\n",
    "        #bias\n",
    "        if b is None:\n",
    "            b = theano.shared(\n",
    "                value = np.zeros( (n_out,),dtype=theano.config.floatX),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "        \n",
    "        #b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        #b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "        \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.output = T.tanh(T.dot(input,self.w)+self.b)\n",
    "        self.params = [self.w,self.b]    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n"
     ]
    }
   ],
   "source": [
    "datasets = load_data('mnist.pkl.gz')\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "# compute number of minibatches for training, validation and testing\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### : Testing the hidden layer ###\n",
    "#------------------------------##\n",
    "\n",
    "x = T.matrix('x')\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "hl = HiddenLayer(rng, input=x, n_in=28*28, n_out=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define MLP class ###\n",
    "class MLP(object):\n",
    "    def __init__(self,rng,input,n_in,n_h,n_out):\n",
    "        self.hidden_layer = HiddenLayer(rng,input=input,n_in=n_in,n_out=n_h)\n",
    "        self.output_layer = LogisticRegression(input=self.hidden_layer.output, n_in=n_h,n_out=n_out)\n",
    "        #regularization\n",
    "        self.L1 = abs(self.hidden_layer.w).sum() + abs(self.output_layer.w).sum()\n",
    "        self.L2 = (self.hidden_layer.w**2).sum() + (self.output_layer.w**2).sum()\n",
    "        # Negative Log Likelihood\n",
    "        self.neg_log_likelihood = (self.output_layer.neg_log_likelihood)\n",
    "        # errors function\n",
    "        self.errors = (self.output_layer.errors)\n",
    "        # params\n",
    "        self.params = self.hidden_layer.params + self.output_layer.params\n",
    "        \n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = T.lscalar('index')\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "# instantiate MLP classifier\n",
    "cl = MLP(rng, input = x, n_in = 28*28, n_h = 500, n_out = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup cost\n",
    "cost = cl.neg_log_likelihood(y) + (cl.L1 * 0.00) + (cl.L2 * 0.0001)\n",
    "\n",
    "# setup gradient\n",
    "gparams = [ T.grad(cost,param) for param in cl.params ]\n",
    "\n",
    "# setup updates \n",
    "updates = [ (param, param - 0.01*gparam) for param,gparam in zip(cl.params,gparams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile training function\n",
    "train = theano.function(inputs=[index],\n",
    "                       outputs=cost,\n",
    "                       updates=updates,\n",
    "                       givens = { x : train_set_x[index * batch_size : (index+1)*batch_size],\n",
    "                                  y : train_set_y[index * batch_size : (index+1)*batch_size]\n",
    "                                }\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0  : cost :  0.754471752453\n",
      "iteration  1  : cost :  0.612203956742\n",
      "iteration  2  : cost :  0.533654163437\n",
      "iteration  3  : cost :  0.48214325281\n",
      "iteration  4  : cost :  0.444899943415\n",
      "iteration  5  : cost :  0.415430178061\n",
      "iteration  6  : cost :  0.390241678427\n",
      "iteration  7  : cost :  0.367454908955\n",
      "iteration  8  : cost :  0.346179195053\n",
      "iteration  9  : cost :  0.326149753193\n",
      "iteration  10  : cost :  0.307374123556\n",
      "iteration  11  : cost :  0.289896807167\n",
      "iteration  12  : cost :  0.273731653996\n",
      "iteration  13  : cost :  0.258860937642\n",
      "iteration  14  : cost :  0.245239175748\n",
      "iteration  15  : cost :  0.232795365595\n",
      "iteration  16  : cost :  0.221439796364\n",
      "iteration  17  : cost :  0.21107495932\n",
      "iteration  18  : cost :  0.201605584501\n",
      "iteration  19  : cost :  0.192944655183\n",
      "iteration  20  : cost :  0.185015434102\n",
      "iteration  21  : cost :  0.177750989743\n",
      "iteration  22  : cost :  0.171092671691\n",
      "iteration  23  : cost :  0.164988446585\n",
      "iteration  24  : cost :  0.159391510685\n",
      "iteration  25  : cost :  0.154259280096\n",
      "iteration  26  : cost :  0.149552708615\n",
      "iteration  27  : cost :  0.145235836546\n",
      "iteration  28  : cost :  0.141275478288\n",
      "iteration  29  : cost :  0.137640982479\n",
      "iteration  30  : cost :  0.134304029303\n",
      "iteration  31  : cost :  0.131238453096\n",
      "iteration  32  : cost :  0.12842008929\n",
      "iteration  33  : cost :  0.125826645289\n",
      "iteration  34  : cost :  0.123437590957\n",
      "iteration  35  : cost :  0.121234060919\n",
      "iteration  36  : cost :  0.1191987606\n",
      "iteration  37  : cost :  0.117315870593\n",
      "iteration  38  : cost :  0.115570947881\n",
      "iteration  39  : cost :  0.113950825737\n",
      "iteration  40  : cost :  0.112443515756\n",
      "iteration  41  : cost :  0.111038115401\n",
      "iteration  42  : cost :  0.109724723346\n",
      "iteration  43  : cost :  0.108494363404\n",
      "iteration  44  : cost :  0.107338916557\n",
      "iteration  45  : cost :  0.106251059782\n",
      "iteration  46  : cost :  0.105224210033\n",
      "iteration  47  : cost :  0.104252471729\n",
      "iteration  48  : cost :  0.103330586375\n",
      "iteration  49  : cost :  0.102453883173\n",
      "iteration  50  : cost :  0.101618229921\n",
      "iteration  51  : cost :  0.10081998383\n",
      "iteration  52  : cost :  0.100055942335\n",
      "iteration  53  : cost :  0.0993232943895\n",
      "iteration "
     ]
    }
   ],
   "source": [
    "# Actual training begins here\n",
    "minibatch_avg_cost = 0\n",
    "for j in xrange(100):\n",
    "    for i in xrange(n_train_batches):\n",
    "        minibatch_avg_cost = train(i)\n",
    "    print 'iteration ',j,' : cost : ', minibatch_avg_cost\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
