{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Stacked Autoencoders with MNIST ##\n",
    "## url : http://deeplearning.net/tutorial/SdA.html ##\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from logistic import load_data,LogisticRegression\n",
    "from mlp import HiddenLayer\n",
    "from DA_module import DenoisingAutoencoder\n",
    "\n",
    "from theano.tensor.shared_randomstreams import RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackedAutoencoder(object):\n",
    "    def __init__(self,rng,hidden_units,n_in=784,n_out=10,corruption_levels=[0.1,0.2,0.3],pre_lr=0.001,fine_lr=0.1):\n",
    "        self.num_layers = len(hidden_units)\n",
    "        self.sigmoid_layers = []\n",
    "        self.dA_layers = []\n",
    "        self.params = []\n",
    "        theano.rng = RandomStreams(rng.randint(2**30))\n",
    "        # input data : x \n",
    "        self.x = T.matrix('x')\n",
    "        # output data : y\n",
    "        self.y = T.ivector('y')\n",
    "        # theano random \n",
    "        theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "        # corruption_levels\n",
    "        self.corruption_levels = corruption_levels\n",
    "        # learning rates\n",
    "        self.pre_lr = pre_lr\n",
    "        self.fine_lr = fine_lr\n",
    "                                \n",
    "        \n",
    "        # setup the layers : input, size_of_input\n",
    "        for i in xrange(self.num_layers):\n",
    "            # if layer 0\n",
    "            if i == 0:# layer 0 : takes n_in inputs                \n",
    "                layer_input_size = n_in\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input_size = hidden_units[i-1]\n",
    "                layer_input = self.sigmoid_layers[-1].output# previous layer's output\n",
    "                # as the list of layers is built step by step\n",
    "                #  the last layer in the list is the previous layer\n",
    "                \n",
    "            #construct a sigmoid layer    \n",
    "            sigmoid_layer = HiddenLayer(rng=rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=layer_input_size,\n",
    "                                        n_out=hidden_units[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "            # add constructed sigmoid layer to list\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            # append params of this layer to params collection of sDA\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "            # setup the corresponding autoencoder layer\n",
    "            da = DenoisingAutoencoder(input=layer_input, rng=rng, num_v = layer_input_size,\n",
    "                                      num_h = hidden_units[i],theano_rng=theano_rng,\n",
    "                                      w=sigmoid_layer.w,bh=sigmoid_layer.b)# param sharing\n",
    "            self.dA_layers.append(da)\n",
    "            \n",
    "        # setupt the last logistic layer\n",
    "        self.output_layer = LogisticRegression(input=self.sigmoid_layers[-1].output,n_in=hidden_units[-1],\n",
    "                                         n_out = n_out)\n",
    "        # add params of logistic layer to global params\n",
    "        # fine_tune cost\n",
    "        self.fine_tune_cost = self.output_layer.neg_log_likelihood(self.y)\n",
    "        # errors\n",
    "        self.errors = self.output_layer.errors(self.y)\n",
    "                    \n",
    "        \n",
    "    def get_pretrain_functions(self,train_set_x,batch_size):\n",
    "        # list of train functions for training DA's \n",
    "        #  layer by layer\n",
    "        fns = [] # build iteratively\n",
    "        for i in xrange(self.num_layers):\n",
    "            # cost and updates of ith DA # corruption level, learning_rate\n",
    "            cost,updates = self.dA_layers[i].step(corruption_level = self.corruption_levels[i],\n",
    "                                             learning_rate = self.pre_lr)\n",
    "            # inputs and other params are already setup while\n",
    "                # building the list of dA's\n",
    "            # train function\n",
    "            index = T.lscalar('index')\n",
    "            train = theano.function([index],cost,updates=updates,\n",
    "                                   givens={ self.x : train_set_x[index*batch_size : (index+1)*batch_size] }\n",
    "                                   )\n",
    "            fns.append(train)            \n",
    "        return fns\n",
    "    \n",
    "    def get_finetune_functions(self,datasets,batch_size):\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches /= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches /= batch_size\n",
    "        \n",
    "        \n",
    "        # index\n",
    "        index = T.lscalar('index')\n",
    "        \n",
    "        # gradients\n",
    "        gparams = T.grad(self.fine_tune_cost,self.params)\n",
    "        # updates\n",
    "        updates = [ (param, param - (gparam * 0.1))\n",
    "                    for param,gparam in zip(self.params,gparams)\n",
    "                    ]\n",
    "        # errors\n",
    "        train = theano.function([index],self.fine_tune_cost,updates=updates,\n",
    "                                    givens = { self.x : train_set_x[index*batch_size : (index+1)*batch_size],\n",
    "                                               self.y : train_set_y[index*batch_size : (index+1)*batch_size]\n",
    "                                              }\n",
    "                                    )\n",
    "        test = theano.function([index],self.errors,\n",
    "                                    givens = { self.x : train_set_x[index*batch_size : (index+1)*batch_size],\n",
    "                                               self.y : train_set_y[index*batch_size : (index+1)*batch_size]\n",
    "                                              }\n",
    "                                    )\n",
    "        \n",
    "        return train,test\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init SDA\n",
    "rng = np.random.RandomState(8437)\n",
    "sda = StackedAutoencoder(rng,[1000,1000,1000],n_in=784,n_out=10,corruption_levels=[0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "datasets = load_data('mnist.pkl.gz')\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "# compute number of minibatches for training, validation and testing\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get pretraining functions\n",
    "ptrain_fns = sda.get_pretrain_functions(train_set_x=train_set_x,batch_size=1)\n",
    "# let us train away\n",
    "#  layer by layer\n",
    "for fn_id in xrange(sda.num_layers):\n",
    "    for iter in xrange(15):\n",
    "        # cost : list of costs\n",
    "        c = []\n",
    "        for batch_id in xrange(n_train_batches):\n",
    "            c.append(ptrain_fns[fn_id](index=batch_id))\n",
    "        print \"Layer : %d, Iteration %d, cost : \" %(fn_id,iter)\n",
    "        print np.mean(c)\n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Supervised Fine Tuning \n",
    "fine_train, fine_test = sda.get_finetune_functions(datasets=datasets,batch_size=20)\n",
    "\n",
    "for iter in xrange(100):\n",
    "    cost_sum = 0\n",
    "    for i in xrange(n_train_batches):\n",
    "        cost = fine_train(index=i)\n",
    "        cost_sum += cost\n",
    "    print \"Iteration %d, Avg Cost : %f\" %(iter,cost_sum/n_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "er_sum=0\n",
    "for i in xrange(n_test_batches):\n",
    "    er = fine_test(index=i)\n",
    "    er_sum += er\n",
    "    print 'Error : %f' %(er)\n",
    "print 'Avg error : %f' %(er_sum/n_test_batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
